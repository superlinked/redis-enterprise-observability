= Redis Enterprise Developer Observability Playbook
:revnumber: 1.0
:docinfo1:

== Introduction

This document provides monitoring guidance for developers running applications
that connect to Redis Enterprise. In particular, this guide focuses on the systems
and resources that are most likely to impact the performance of your application.

.Dashboard showing relevant statistics for a Node
image::images/node_summary.png[]

To effectively monitor a Redis Enterprise cluster, you need to observe
core cluster resources and key database performance indicators.

Core cluster resources include:

* <<Memory, Memory utilization>>
* <<CPU, CPU utilization>>
* <<Connections, Database connections>>

Key database performance indicators include:

* <<Latency>>
* <<Cache hit rate>>
* <<Key eviction rate>>

.Dashboard showing an overview of cluster metrics
image::images/cluster_overview.png[]


== Core cluster resource monitoring

=== Memory

Every Redis Enterprise database has a maximum configured memory limit to ensure isolation
in a multi-database cluster.

[cols="1,1,1"]
|===
| *Metric name* | *Definition* | *Unit*

| memory_usage_percent | Used memory / memory limit for database | Percentage
|===

image::images/playbook_used-memory.png[]

==== Thresholds

The appropriate memory threshold depends on how the application is using Redis.

* <<Caching workloads>>, which permit Redis to evict key, can safely use 100% of memory.
* <<Non-caching workloads>> do not permit key eviction and should be closely monitored as soon as memory usage reaches 80%.

==== Caching workloads

For applications using Redis solely as a cache, you can safely let the memory usage
reach 100% as long as you have an https://redis.io/blog/cache-eviction-strategies/[eviction policy] in place. This will ensure
that Redis can evict keys while continuing to accept new writes.

While your Redis database is using 100% of available memory, it's important to monitor
performance.

* https://redis.io/docs/latest/operate/oss_and_stack/management/optimization/latency/[Latency]
+
Average latency should be at or below 1 ms.

* Cache miss ratio should be below 50%
+
(100 * bdb_read_hits / (bdb_read_hits + bdb_read_misses)) < 50
+
The ideal cache miss ratio depends on the application, but generally, the ratio should fall below 50%.
High miss ratios coupled with high numbers of object evictions may indicate that your cache is too small.
This can cause thrashing on the application side, where the cache is constantly being invalidated.

* https://redis.io/docs/latest/develop/reference/eviction/[Evicted keys]
+
bdb_evicted_objects > 1
+
Keys are being evicted from your database. In some caching use scenarios this may be acceptable, so please disable this check if you meet such requirements.

==== Non-caching workloads

If no eviction policy is enabled, then Redis will stop accepting writes once memory reaches 100%.
Therefore, for non-caching workloads, we recommend that you configure an alert at 80% memory usage.
Once your database reaches this 80% threshold, you should closely review the rate of memory usage growth.

==== Troubleshooting

|===
|*Issue* |*Possible causes*

|Possible spike in activity
|Check both the Network Traffic and Operations Per Second metrics to determine if there is a corresponding increase

|Database sized incorrectly
|View the Memory Usage raw bytes over time to see if a usage pattern has changed

|Incorrect retention policies
|Check to see if keys are being Evicted or Expired
|===

==== Remediation

|===
|*Action*|*Method*

|Increase database memory
|Raise the database memory limit using the Redis Enterprise console or its API.

|Modify eviction policy
|For workloads that can tolerate evicted keys (e.g., caching), configure a global eviction policy for your database.

|Modify key retention policy
|Determine whether the application can set a reasonable TTL (time-to-live) on some or all
of the data written to Redis.
|===

=== CPU

Redis Enterprise provides several CPU metrics:

[cols="1,1,1"]
|===
| *Metric name* | *Definition* | *Unit*

| Shard CPU | CPU time portion spent by database shards  | Percentage, up to 100% per shard
| Proxy CPU | CPU time portion spent by the cluster's proxy(s) | Percentage, 100% per proxy thread
| Node CPU (User and System) | CPU time portion spent by all user-space and kernel-level processes | Percentage, 100% per node CPU
|===

To understand CPU metrics, it's worth recalling how a Redis Enterprise cluster is organized.
A cluster consists of one or more nodes. Each node is a VM (or cloud compute instance) or
a bare-metal server.

A database is a set of processes, known as shards, deployed across the nodes of a cluster.

In the dashboard, shard CPU is the CPU utilization of the processes that make up the database.
When diagnosing performance issues, start by looking at shard CPU.

.Dashboard displaying CPU usage
image::images/playbook_database-cpu.png[]

==== Thresholds

In general, we define high CPU as any CPU utilization above 80% of total capacity.

Shard CPU should remain below 80%. Shards are single-threaded, so a shard CPU of 100% means that the shard is fully utilized.

.Display showing DMC Proxy CPU usage
image::images/playbook_proxy-cpu-usage.png[]

Proxy CPU should remain below 80% of total capacity.
The proxy is a multi-threaded process that handles client connections and forwards requests to the appropriate shard.
Because the total number of proxy threads is configurable, the proxy CPU may exceed 100%.
A proxy configured with 6 threads can reach 600% CPU utilization, so in this case,
keeping utilization below 80% means keeping the total proxy CPU usage below 480%.

.Dashboard displaying an ensemble of Node CPU usage data
image::images/node_cpu.png[]

Node CPU should also remain below 80% of total capacity. As with the proxy, the node CPU is variable depending
on the CPU capacity of the node. You will need to calibrate your alerting based on the number of cores in your nodes.

==== Troubleshooting

High CPU utilization has multiple possible causes. Common causes include an under-provisioned cluster,
excess inefficient Redis operations, and hot master shards.

|===
|*Issue*|*Possible causes*|*Remediation*

|High CPU utilization across all shards of a database
|This usually indicates that the database is under-provisioned in terms of number of shards.
A secondary cause may be that the application is running too many inefficient Redis operations.
You can detect slow Redis operations by enabling the slow log in the Redis Enterprise UI.
|First, rule out inefficient Redis operations as the cause of the high CPU utilization.
See <<Slow operations>> for details on this.
If inefficient Redis operations are not the cause, then increase the number of shards in the database.

|High CPU utilization on a single shard, with the remaining shards having low CPU utilization
|This usually indicates a master shard with at least one hot key.
Hot keys are keys that are accessed extremely frequently (e.g., more than 1000 times per second).
|Hot key issues generally cannot be resolved by increasing the number of shards.
To resole this issue, see <<Hot keys>>.

|High Proxy CPU
|There are several possible causes of high proxy CPU.
First, review the behavior of connections to the database.
Frequent cycling of connections, especially with TLS is enabled, can cause high proxy CPU utilization.
This is especially true when you see more than 100 connections per second per thread.
Such behavior is almost always a sign of a misbehaving application.

Seconds, review the total number of operations per second against the cluster.
If you see more than 50k operations per second per thread, you may need to increase the number of proxy threads.
|In the case of high connection cycling, review the application's connection behavior.

In the case of high operations per second, https://redis.io/docs/latest/operate/rs/references/cli-utilities/rladmin/tune/#tune-proxy[increase the number of proxy threads].

|High Node CPU
|You will typically detect high shard or proxy CPU utilization before you detect high node CPU utilization.
Use the remediation steps above to address high shard and proxy CPU utilization.
In spite of this, if you see high node CPU utilization, you may need to increase the number of nodes in the cluster.
|Consider increasing the number of nodes in the cluster and the rebalancing the shards across the new nodes.
This is a complex operation and should be done with the help of Redis support.

|High System CPU
|Most of the issues above will reflect user-space CPU utilization.
However, if you see high system CPU utilization, this may indicate a problem at the network or storage level.
|Review network bytes in and network bytes out to rule out any unexpected spikes in network traffic.
You may need perform some deeper network diagnostics to identify the cause of the high system CPU utilization.
For example, with high rates of packet loss, you may need to review network configurations or even the network hardware.
|===

=== Connections

The Redis Enterprise database dashboard indicates to the total number of connections to the database over time.

This connection count metric should be monitored with both a minimum and maximum number of connections in mind.
Based on the number of application instances connecting to Redis (and whether your application uses connection pooling),
you should have a rough idea of the minimum and maximum number of connections you expect to see for any given database.
This number should remain relatively constant over time.

|===
|*Issue*|*Possible causes*|*Remediation*

|Minimum number of clients not met
|Incorrect client configuration, network firewall or network issues
|Check client configurations and firewall settings

|Maximum connections exceeded
|Your application may not be releasing connections
|Review the application's connection configuration.

|Erratic connection counts (e.g, spikes and drops)
|===

.Dashboard displaying connections
image::images/playbook_bdb-connections.png[]

==== Troubleshooting

|===
|*Issue*|*Possible causes*

|No client connections
|Poorly configured client

|Too many connections
|Poorly configured client
|===

==== Remediation

|===
|*Action*|*Method*

|Clients Misconfigured
|Confirm client configurations

|Networking issue
|From a client node TELNET to the endpoint and issue the PING command

|Too many connections
|Be sure that you are using pooling on your client library and that your pools are sized according

|Too many connections
|Using rladmin run "tune proxy PROXY_NUMBER threads VALUE"
|===

== Database performance indicators

=== Latency

Definition
redis_enterprise.avg_latency (unit: microseconds)

This is the average amount of time that a request takes to return from the time that it first hits the Redis Enterprise proxy until the response is returned.  It does not include the full time from the remote client’s perspective.

.Dashboard display of latency metrics
image::images/playbook_bdb-latency.png[]

==== Monitoring notes

Due to the fact that Redis is popular due to performance, generally you would expect most operations to return in single digit milliseconds.  Tune any alerts to match your SLA.  It is generally recommended that you also measure Redis operation latency at the client side to make it easier to determine if a server slow down or an increase in network latency is the culprit in any performance issues.

.Display showing a noticeable spike in latency
image::images/latency_spike.png[]

==== Troubleshooting

|===
|*Issue*|*Possible causes*

|Possible spike in requests
|Check both the Network Traffic and Operations Per Second metrics to determine if there is a corresponding increase

|Slow Running queries
|Check the slow log in the Redis Enterprise UI for the database

|Insufficient compute resources
|Check to see if the CPU Usage, Memory Usage Percentage, or Evictions are increasing
|===

.Cloud console slow log display
image::images/slow_log.png[]

==== Remediation
|===
|*Action*|*Method*

|Increase resources
|The database can be scaled up online by going to the Web UI and enabling clustering on the database.  In extreme cases more nodes can be added to the cluster and resources rebalanced.

|Inefficient Queries
|Redis allows you to view a slow log with a tunable threshold.  It can be viewed either in the Redis Enterprise UI or by running

redis-cli -h HOST -p PORT -a PASSWORD SLOWLOG GET 100
|===

=== Cache hit rate
Definition
redis_enterprise.cache_hit_rate (unit: percent)

This is the percentage of time that Redis is accessing a key that already exists.

==== Monitoring notes
This metric is useful only in the caching use case and should be ignored for all other use cases.
There are tradeoffs between the freshness of the data in the cache and efficacy of the cache mitigating traffic to any backend data service.
https://redis.io/glossary/cache-miss/[Cache misses] can have consequential impacts on system performance.
These tradeoffs should be considered carefully when determining the threshold for alerting.

.Dashboard showing Hit/Miss ratios
image::images/playbook_read-write-misses.png[]

==== Troubleshooting
This is highly specific to the application caching with no general rules that are applicable in the majority of cases.

|===
|*Issue*|*Possible causes*

|Low hit rate
|Data is being evicted due to TTL policy
|===

==== Remediation
Note that redis commands return information on whether or not a key or field already exists.  For example, HSET command returns the number of fields in the hash that were added.

|===
|*Action*|*Method*

|Monitor activity
|Check return values to determine if values were added
|===

=== Key eviction rate

Definition
redis_enterprise.evicted_objects (unit: count)

This is the count of items that have been evicted from the database.

==== Monitoring notes
Eviction occurs when the database is close to capacity.  In this condition, the eviction policy starts to take effect.  While Expiration is fairly common in the caching use case, Eviction from the cache should generally be a matter of concern.  At very high throughput and very restricted resource use cases, sometimes the eviction sweeps cannot keep up with memory pressure.  Relying on Eviction as a memory management technique should be considered carefully.

.Dashboard displaying object evictions
image::images/playbook_eviction-expiration.png[]

==== Troubleshooting
While memory usage and network traffic will not help you pinpoint a root cause, network traffic is an excellent leading indicator of trouble.  Changes in network traffic patterns indicate corresponding changes in database behavior and further investigation is usually warranted.

|===
|*Issue*|*Possible causes*

|See Memory Usage Percentage Possible Causes
|===

==== Remediation
See Memory Usage Percentage Remediation

|===
|*Action*|*Method*

|===

See Memory Usage Percentage Remediation
Secondary Indicators
Network Traffic
redis_enterprise.ingress_bytes/redis_enterprise.egress_bytes (unit: bytes)
Counters for the network traffic coming into the database and out from the database

== Data access anti-patterns

=== Slow operations

Any Redis operation whose performance is O(n) (or worse) can cause high CPU utilization.
Examples include https://redis.io/docs/latest/commands/keys/[KEYS], https://redis.io/docs/latest/commands/smembers/[SMEMBERS],
and https://redis.io/docs/latest/commands/lrem/[LREM]. In additionl

=== Hot keys

== Alerting

Included in the Redis Observability package is a suite of alerts and their associated tests for use with Prometheus.
There are https://github.com/redis-field-engineering/redis-enterprise-observability/tree/main/grafana#alerts[alerts] for
all of the metrics described in both the Resources and the Performance sections below.

Not all the alerts are appropriate for all environments; installations that do not use persistence have no need of storage
alerts, for example.

There are https://github.com/redis-field-engineering/redis-enterprise-observability/tree/main/grafana/tests[tests] included
with the alerts that validate the individual triggers. These can be used to validate any modifications made when tailoring the
alerts for specific environments and/or use cases.

In order to trigger alerts it is necessary to install Prometheus' https://prometheus.io/docs/alerting/latest/configuration/[Alertmanager].
Grafana have published a fairly straightforward guide should you wish for more comprehensive https://grafana.com/blog/2020/02/25/step-by-step-guide-to-setting-up-prometheus-alertmanager-with-slack-pagerduty-and-gmail/[instructions] than what are provided below.

There are two sections that must be configured, and by and large they can use the default values. The first section should be un-commented, eg. everything below Alertmanager configuration. This will start the Alertmanager and instruct it to listen on its default port of 9093.

[subs="+quotes"]
----
# Alertmanager configuration
#alerting:
#  alertmanagers:
#    - static_configs:
#        - targets:
#          - alertmanager:9093
----

The second section instructs the Alertmanager to read specific rules files. If you pasted the 'rules.yml' file into '/etc/prometheus' then the following configuration would be required.

[subs="+quotes"]
----
# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
  - "error_rules.yml"
  - "rules.yml"
----

Once this is done you should restart Prometheus.
The built-in configuration, error_rules.yml, has a single alert: Critical Connection Exception.
If you open the Prometheus console, by default located at port 9090, and select the Alert tab you should see it.

The following is a list of alerts contained in the alerts.yml file. There are several points to be considered:

- Not all installations have all metrics
- Most metrics only alert if the specified trigger persists for a given duraction

|===
|Name |Trigger

|Average Latency Warning
|round(bdb_avg_latency * 1000) > 1

|Average Latency Critical
|round(bdb_avg_latency * 1000) > 4

|No Redis Connections
|bdb_conns < 1

|Excessive Connections
|bdb_conns > 64000

|No Redis Requests
|bdb_total_req < 1

|Excessive Redis Requests
|bdb_total_req > 1000000

|DB full
|round((bdb_used_memory/bdb_memory_limit) * 100) > 98

|DB full in 2 hours
|round((bdb_used_memory/bdb_memory_limit) * 100) < 98 and (predict_linear(bdb_used_memory[15m], 2 * 3600) / bdb_memory_limit) > 0.3 and round(predict_linear(bdb_used_memory[15m], 2 * 3600)/bdb_memory_limit) > 0.98

|Low Hit Ratio
|(100 * bdb_read_hits)/(bdb_read_hits + bdb_read_misses) < 50

|Unexpected Object Eviction
|bdb_evicted_objects > 1

|ReplicaSync Status
|bdb_replicaof_syncer_status > 0

|CRDTSync Status
|bdb_crdt_syncer_status > 0

|ReplicaLag Status
|bdb_replicaof_syncer_local_ingress_lag_time > 500

|CRDTLag Status
|bdb_crdt_syncer_local_ingress_lag_time > 500

|Node Not Responding
|count(node_up) != 3

|Node Persistent Storage
|round((node_persistent_storage_free/node_persistent_storage_avail) * 100) <= 5

|Node Ephemeral Storage
|round((node_ephemeral_storage_free/node_ephemeral_storage_avail) * 100) <= 5

|Node Free Memory
|round((node_available_memory/node_free_memory) * 100) <= 15

|Node CPU Usage
|round((1 - node_cpu_idle) * 100) >= 80

|Shard Down
|redis_up == 0

|Master Shard Down
|floor(redis_master_link_status{role="slave"}) < 1

|Shard CPU Usage
|redis_process_cpu_usage_percent >= 80

|Hot Master Shard
|redis_process_cpu_usage_percent{role="master"} > 0.75 and redis_process_cpu_usage_percent{role="master"} > on (bdb) group_left() (avg by (bdb)(redis_process_cpu_usage_percent{role="master"}) + on(bdb) 1.2 * stddev by (bdb) (redis_process_cpu_usage_percent{role="master"}))

|ShardConnectionCount
|redis_connected_clients > 500
|===


== Appendix A: Grafana Dashboards

Redis' Grafana dashboards are available tailored for Software(Enterprise) and Cloud installations.
They come in two flavors: basic, which has no third-party dependencies; and extended, which requires a third-party library to perform ReST calls.

There are two additional sets for software that provide drill-down functionality; the workflow dashboards.
Lastly there are legacy dashboards which can be found in Classic.

=== Software
- https://github.com/redis-field-engineering/redis-enterprise-observability/tree/playbook/grafana/dashboards/software/basic[Basic]
- https://github.com/redis-field-engineering/redis-enterprise-observability/tree/playbook/grafana/dashboards/software/extended[Extended]
- https://github.com/redis-field-engineering/redis-enterprise-observability/tree/playbook/grafana/dashboards/software/classic[Classic]

==== Workflow
- https://github.com/redis-field-engineering/redis-enterprise-observability/tree/playbook/grafana/dashboards/workflow/databases[Database]
- https://github.com/redis-field-engineering/redis-enterprise-observability/tree/playbook/grafana/dashboards/workflow/nodes[Node]

=== Cloud
- https://github.com/redis-field-engineering/redis-enterprise-observability/tree/playbook/grafana/dashboards/cloud/basic[Basic]
- https://github.com/redis-field-engineering/redis-enterprise-observability/tree/playbook/grafana/dashboards/cloud/extended[Exteneded]
